{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ochoa-josue/techwise-deepfake-classification/blob/feature%2Fgan-model/gan_discriminator_deepfake_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HQOv309EpBkn"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet tensorflow matplotlib tensorflow-datasets ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9HkbfG52pMPq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory #load image datasets\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, load_model #sequential api for generator and discriminator\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling2D, BatchNormalization\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.metrics import AUC #area under the curve for model performance (especially imbalanced datasets)\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pma4qS78nxM",
        "outputId": "a8ba163f-20a7-4179-f1a5-6e1c5f53b861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWfFq3ojsVyn"
      },
      "source": [
        "### **Retrieve and Preprocess Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNGHH8DHpaWi",
        "outputId": "ca4a116e-5257-49fa-9d10-ef244d6589a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "extracted files: ['valid', 'test', 'train']\n",
            "['valid', 'test', 'train']\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "  drive.mount('/content/drive')\n",
        "# os.chdir(\"/content/drive/My Drive/conforming-images-subset\") #change current working directory to specified directory\n",
        "\n",
        "zip_file_path = '/content/drive/My Drive/conforming_images_subset.zip'\n",
        "extracted_dir_path = '/content/conforming-images-subset/'\n",
        "os.makedirs(extracted_dir_path, exist_ok=True) #creates dir if non-existent\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "  zip_ref.extractall(extracted_dir_path)\n",
        "\n",
        "extracted_files = os.listdir(extracted_dir_path)\n",
        "print(f\"extracted files: {extracted_files}\")\n",
        "print(extracted_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M938AyIzqbP5"
      },
      "outputs": [],
      "source": [
        "# train_dir = \"/content/drive/My Drive/conforming-images-subset/train\"\n",
        "# valid_dir = \"/content/drive/My Drive/conforming-images-subset/valid\"\n",
        "# test_dir = \"/content/drive/My Drive/conforming-images-subset/test\"\n",
        "\n",
        "train_dir = os.path.join(extracted_dir_path, 'train')\n",
        "valid_dir = os.path.join(extracted_dir_path, 'valid')\n",
        "test_dir = os.path.join(extracted_dir_path, 'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "g-dUuF9ptZnm"
      },
      "outputs": [],
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.05),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomZoom(0.05),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomTranslation(height_factor=0.05, width_factor=0.05),\n",
        "])\n",
        "\n",
        "def preprocess(images, labels):\n",
        "  images = tf.cast(images, tf.float32) #convert tensor img to tf.float32 type\n",
        "  images = images / 255.0 #normalize data between 0 to 1\n",
        "\n",
        "  return images, labels\n",
        "\n",
        "def preprocess_with_augmentation(images, labels):\n",
        "  images = data_augmentation(images, training=True)\n",
        "  images = tf.cast(images, tf.float32) #convert tensor img to tf.float32 type\n",
        "  images = images / 255.0 #normalize data between range 0 to 1\n",
        "  return images, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLbtXDiayObL",
        "outputId": "c475df8f-3b37-414c-fbc7-38511b51e7a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_dataset = image_dataset_from_directory(train_dir, label_mode='binary', image_size=(256,256), batch_size=32) #image anomaly detection\n",
        "valid_dataset = image_dataset_from_directory(valid_dir, label_mode='binary', image_size=(256,256), batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(test_dir, label_mode='binary', image_size=(256,256), batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mCjfn1vysSC2"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.map(preprocess_with_augmentation)\n",
        "valid_dataset = valid_dataset.map(preprocess)\n",
        "test_dataset = test_dataset.map(preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eBUHqtV6Qe1"
      },
      "source": [
        "### **Build GAN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Z2qn1BlwvEay"
      },
      "outputs": [],
      "source": [
        "class GAN(tf.keras.Model):\n",
        "  def __init__(self, generator, discriminator):\n",
        "    super(GAN, self).__init__() #invoking constructor of tf.keras.Model to initialize GAN proxy object\n",
        "    self.generator = generator\n",
        "    self.discriminator = discriminator\n",
        "\n",
        "  def compile(self, g_optimizer, d_optimizer, loss_function):\n",
        "    super(GAN, self).compile() #ensure compilation of parent class is executed\n",
        "    self.g_optimizer = g_optimizer\n",
        "    self.d_optimizer = d_optimizer\n",
        "    self.loss_function = loss_function\n",
        "\n",
        "  def train_step(self, real_images): #override parent train_step when calling to TensorFlow fit function\n",
        "    batch_size = tf.shape(real_images)[0] #retrieving batch size\n",
        "    noise_dim = 128 #128 dimensions\n",
        "\n",
        "    #generate fake images\n",
        "    random_noise = tf.random.normal(shape=(batch_size, noise_dim)) #create random noise vectors with Gaussian distribution\n",
        "    fake_images = self.generator(random_noise, training=True)\n",
        "\n",
        "    #train discriminator\n",
        "    with tf.GradientTape() as disc_tape: #records operations on tensors; thereafter performs automatic differentiation, in other words, compute derivatives or gradients of functions\n",
        "        real_output = self.discriminator(real_images, training=True)\n",
        "        fake_output = self.discriminator(fake_images, training=True)\n",
        "\n",
        "        #calculate loss\n",
        "        real_loss = self.loss_function(tf.ones_like(real_output), real_output)\n",
        "        fake_loss = self.loss_function(tf.zeros_like(fake_output), fake_output)\n",
        "\n",
        "        #total discriminator loss\n",
        "        disc_loss = real_loss + fake_loss\n",
        "\n",
        "    #calculate gradients for discriminator\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
        "\n",
        "    #apply gradients to the optimizer\n",
        "    self.d_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
        "\n",
        "    #train generator\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "      fake_images = self.generator(random_noise, training=True) #generate fake images\n",
        "      fake_output = self.discriminator(fake_images, training=True) #disc predict fake images\n",
        "      gen_loss = self.loss_function(tf.ones_like(fake_output), fake_output) #show generator loss; maximimize loss to optimize generator\n",
        "\n",
        "    #calculate gradients for generator\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
        "    #apply gradients to the optimizer\n",
        "    self.g_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
        "\n",
        "    return {'disc_loss': disc_loss, 'gen_loss': gen_loss}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TdePdiVH6sul"
      },
      "outputs": [],
      "source": [
        "def build_generator():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(7*7*128, input_dim=128)) #intakes 128-dimesional input of random noise to output tensor shape\n",
        "  model.add(LeakyReLU(0.2)) #introduce non-linearlity for continued learning\n",
        "  model.add(BatchNormalization(momentum=0.8)) #stabilize training process by tracking moving averages overtime; faster convergence\n",
        "  model.add(Reshape((7, 7, 128))) #reshapes into 4D tensor which inclues batch size, height, width, and channels\n",
        "\n",
        "  #upsampling block 1\n",
        "  model.add(UpSampling2D()) #double dimensions of the input tensor for higher resolutions from low-dimensional noise vectors\n",
        "  model.add(Conv2D(128, 5, padding='same')) #adds a convolutional layer with 256 filters, 5x5 kernel size, and same padding to preserve spatial dimensions as input image\n",
        "  model.add(LeakyReLU(0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  #upsampling block 2\n",
        "  model.add(UpSampling2D())\n",
        "  model.add(Conv2D(64, 5, padding='same')) #reduce filters to prevent overfitting and allow feature abstraction e.g. capturing simple patterns (edges, textures)\n",
        "  model.add(LeakyReLU(0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  #simplify model\n",
        "  #convolution block 1\n",
        "  model.add(Conv2D(128, 4, padding='same'))\n",
        "  model.add(LeakyReLU(0.2))\n",
        "\n",
        "  #output layer adjustment; output layer is tanh activation\n",
        "  model.add(Conv2D(3, 4, padding='same', activation='sigmoid')) #sigmoid activation for data normalized between 0 and 1 for RGB images\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Y53-_E657oGG"
      },
      "outputs": [],
      "source": [
        "def build_discriminator(input_shape=(256, 256, 3)):\n",
        "  model = Sequential([\n",
        "      #convolution block 1\n",
        "      Conv2D(32, kernel_size=5, strides=2, input_shape=input_shape, padding=\"same\", kernel_regularizer=l2(0.01)), #stride reduces spatial dimensions by half\n",
        "      LeakyReLU(alpha=0.2),\n",
        "      Dropout(0.3), #prevent overfitting where 30% of input units or features are randomly set to zero\n",
        "\n",
        "      #convolution block 2\n",
        "      Conv2D(64, kernel_size=5, strides=2, padding=\"same\", kernel_regularizer=l2(0.01)),\n",
        "      LeakyReLU(alpha=0.2),\n",
        "      Dropout(0.3),\n",
        "\n",
        "      #simplify model\n",
        "      #convolution block 3\n",
        "      Conv2D(256, kernel_size=5, strides=2, padding=\"same\"),\n",
        "      LeakyReLU(alpha=0.2),\n",
        "      Dropout(0.3),\n",
        "\n",
        "      #flatten and output layer\n",
        "      Flatten(), #flatten into a 1D tensor for easier integration with fully connected layers\n",
        "      Dense(1, kernel_regularizer=l2(0.01)) #indicate whether image is real or fake\n",
        "  ])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf9CmNVbvyyW"
      },
      "source": [
        "### **Data Visualization and Compiling GAN Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7rUHL8N6Sj9",
        "outputId": "5794bc8e-7b0d-44a6-a1ae-e72d14127630"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.8920 - accuracy: 0.5254 - auc: 0.5842"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 3160s 5s/step - loss: 0.8920 - accuracy: 0.5254 - auc: 0.5842 - val_loss: 0.7599 - val_accuracy: 0.5170 - val_auc: 0.5850\n",
            "Epoch 2/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.7272 - accuracy: 0.5609 - auc: 0.6252"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 3112s 5s/step - loss: 0.7272 - accuracy: 0.5609 - auc: 0.6252 - val_loss: 0.7055 - val_accuracy: 0.5275 - val_auc: 0.6148\n",
            "Epoch 3/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.6956 - accuracy: 0.5807 - auc: 0.6411"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 3171s 5s/step - loss: 0.6956 - accuracy: 0.5807 - auc: 0.6411 - val_loss: 0.6910 - val_accuracy: 0.5552 - val_auc: 0.6198\n",
            "Epoch 4/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.6803 - accuracy: 0.5938 - auc: 0.6520"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 3195s 5s/step - loss: 0.6803 - accuracy: 0.5938 - auc: 0.6520 - val_loss: 0.6760 - val_accuracy: 0.5615 - val_auc: 0.6330\n",
            "Epoch 5/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.6688 - accuracy: 0.6080 - auc: 0.6671"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 3117s 5s/step - loss: 0.6688 - accuracy: 0.6080 - auc: 0.6671 - val_loss: 0.6609 - val_accuracy: 0.5853 - val_auc: 0.6618\n",
            "Epoch 6/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.6628 - accuracy: 0.6141 - auc: 0.6751"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 3114s 5s/step - loss: 0.6628 - accuracy: 0.6141 - auc: 0.6751 - val_loss: 0.6528 - val_accuracy: 0.6345 - val_auc: 0.6965\n",
            "Epoch 7/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.6599 - accuracy: 0.6218 - auc: 0.6786"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 3113s 5s/step - loss: 0.6599 - accuracy: 0.6218 - auc: 0.6786 - val_loss: 0.6475 - val_accuracy: 0.6202 - val_auc: 0.6846\n",
            "Epoch 8/50\n",
            "625/625 [==============================] - 3114s 5s/step - loss: 0.6589 - accuracy: 0.6216 - auc: 0.6780 - val_loss: 0.6509 - val_accuracy: 0.6012 - val_auc: 0.6670\n",
            "Epoch 9/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.6529 - accuracy: 0.6277 - auc: 0.6873"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 3104s 5s/step - loss: 0.6529 - accuracy: 0.6277 - auc: 0.6873 - val_loss: 0.6422 - val_accuracy: 0.6112 - val_auc: 0.6834\n",
            "Epoch 10/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.6481 - accuracy: 0.6345 - auc: 0.6938"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 3144s 5s/step - loss: 0.6481 - accuracy: 0.6345 - auc: 0.6938 - val_loss: 0.6415 - val_accuracy: 0.6378 - val_auc: 0.6953\n",
            "Epoch 11/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.6431 - accuracy: 0.6447 - auc: 0.6988"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 3107s 5s/step - loss: 0.6431 - accuracy: 0.6447 - auc: 0.6988 - val_loss: 0.6361 - val_accuracy: 0.6503 - val_auc: 0.7114\n",
            "Epoch 12/50\n",
            "625/625 [==============================] - 3107s 5s/step - loss: 0.6412 - accuracy: 0.6466 - auc: 0.7040 - val_loss: 0.6513 - val_accuracy: 0.5905 - val_auc: 0.6616\n",
            "Epoch 13/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.6414 - accuracy: 0.6448 - auc: 0.7019"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 3127s 5s/step - loss: 0.6414 - accuracy: 0.6448 - auc: 0.7019 - val_loss: 0.6270 - val_accuracy: 0.6553 - val_auc: 0.7187\n",
            "Epoch 14/50\n",
            "625/625 [==============================] - 3125s 5s/step - loss: 0.6357 - accuracy: 0.6496 - auc: 0.7079 - val_loss: 0.6283 - val_accuracy: 0.6267 - val_auc: 0.7012\n",
            "Epoch 15/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.6333 - accuracy: 0.6532 - auc: 0.7102"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 3119s 5s/step - loss: 0.6333 - accuracy: 0.6532 - auc: 0.7102 - val_loss: 0.6157 - val_accuracy: 0.6628 - val_auc: 0.7343\n",
            "Epoch 16/50\n",
            "625/625 [==============================] - 3087s 5s/step - loss: 0.6296 - accuracy: 0.6605 - auc: 0.7180 - val_loss: 0.6281 - val_accuracy: 0.6202 - val_auc: 0.6954\n",
            "Epoch 17/50\n",
            "625/625 [==============================] - 3122s 5s/step - loss: 0.6281 - accuracy: 0.6630 - auc: 0.7212 - val_loss: 0.6162 - val_accuracy: 0.6492 - val_auc: 0.7179\n",
            "Epoch 18/50\n",
            "625/625 [==============================] - 3201s 5s/step - loss: 0.6274 - accuracy: 0.6636 - auc: 0.7189 - val_loss: 0.6204 - val_accuracy: 0.6447 - val_auc: 0.7023\n",
            "Epoch 19/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.6236 - accuracy: 0.6682 - auc: 0.7241"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 3136s 5s/step - loss: 0.6236 - accuracy: 0.6682 - auc: 0.7241 - val_loss: 0.6040 - val_accuracy: 0.6628 - val_auc: 0.7270\n",
            "Epoch 20/50\n",
            "625/625 [==============================] - 3132s 5s/step - loss: 0.6197 - accuracy: 0.6694 - auc: 0.7286 - val_loss: 0.6185 - val_accuracy: 0.6690 - val_auc: 0.7301\n",
            "Epoch 21/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.6237 - accuracy: 0.6685 - auc: 0.7258"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "625/625 [==============================] - 3118s 5s/step - loss: 0.6237 - accuracy: 0.6685 - auc: 0.7258 - val_loss: 0.5979 - val_accuracy: 0.6852 - val_auc: 0.7447\n",
            "Epoch 22/50\n",
            "625/625 [==============================] - 3111s 5s/step - loss: 0.6193 - accuracy: 0.6764 - auc: 0.7314 - val_loss: 0.6078 - val_accuracy: 0.6557 - val_auc: 0.7245\n",
            "Epoch 23/50\n",
            "625/625 [==============================] - 3118s 5s/step - loss: 0.6200 - accuracy: 0.6741 - auc: 0.7274 - val_loss: 0.6015 - val_accuracy: 0.6845 - val_auc: 0.7361\n",
            "Epoch 24/50\n",
            "625/625 [==============================] - 3127s 5s/step - loss: 0.6159 - accuracy: 0.6809 - auc: 0.7339 - val_loss: 0.6009 - val_accuracy: 0.6762 - val_auc: 0.7348\n",
            "Epoch 25/50\n",
            "625/625 [==============================] - 3143s 5s/step - loss: 0.6141 - accuracy: 0.6766 - auc: 0.7351 - val_loss: 0.6010 - val_accuracy: 0.6635 - val_auc: 0.7272\n",
            "Epoch 26/50\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.6132 - accuracy: 0.6847 - auc: 0.7400"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r625/625 [==============================] - 3192s 5s/step - loss: 0.6132 - accuracy: 0.6847 - auc: 0.7400 - val_loss: 0.5920 - val_accuracy: 0.6810 - val_auc: 0.7394\n",
            "Epoch 27/50\n",
            "625/625 [==============================] - 3123s 5s/step - loss: 0.6123 - accuracy: 0.6851 - auc: 0.7407 - val_loss: 0.6029 - val_accuracy: 0.6653 - val_auc: 0.7250\n",
            "Epoch 28/50\n",
            "101/625 [===>..........................] - ETA: 42:05 - loss: 0.6112 - accuracy: 0.6835 - auc: 0.7399"
          ]
        }
      ],
      "source": [
        "# generator = build_generator()\n",
        "# g_optimizer = tf.keras.optimizers.Adam(1e-4) #find best convergence along with optimal learning rate\n",
        "# noise_dim = 128\n",
        "# gan_model = GAN(generator, discriminator, noise_dim) #create GAN\n",
        "# gan_model.compile(g_optimizer=g_optimizer, d_optimizer=d_optimizer) #compile GAN\n",
        "# gan_model.fit(train_dataset, epochs=50, validation_data=valid_dataset, callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "d_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "discriminator.compile(optimizer=d_optimizer, loss=loss_function, metrics=['accuracy', AUC(name='auc')])\n",
        "\n",
        "checkpoint_cb = ModelCheckpoint(\"best_discriminator\", save_best_only=True, monitor='val_loss') #callback to save the best model checkpoint during training\n",
        "# early_stopping_cb = EarlyStopping(patience=20, restore_best_weights=True, monitor='val_loss') #stop training if no improvement at 10 consecutive epochs\n",
        "early_stopping_cb = EarlyStopping(patience=10, restore_best_weights=True, monitor='val_loss') #for 50 epochs\n",
        "tensorboard_cb = TensorBoard(log_dir='/content/drive/My Drive/logs', histogram_freq=1) #TensorBoard callback to visualize model performance\n",
        "\n",
        "#train the discriminator on the dataset -\n",
        "# history = discriminator.fit(train_dataset, epochs=100, validation_data=valid_dataset, callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb])\n",
        "history = discriminator.fit(train_dataset, epochs=50, validation_data=valid_dataset, callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb])\n",
        "\n",
        "test_loss, test_accuracy = discriminator.evaluate(test_dataset) #evaluate model performance for unseen data\n",
        "print(f\"test accuracy: {test_accuracy*100:.2f}%\")\n",
        "\n",
        "discriminator.save('final_discriminator_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YU2FXT3xpDyY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "#visualize AUC\n",
        "plt.plot(history.history['auc'], label='Training AUC')\n",
        "plt.plot(history.history['val_auc'], label='Validation AUC')\n",
        "plt.title('Training and Validation AUC')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#visualize training process\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#evaluate model performance\n",
        "train_loss, train_accuracy = discriminator.evaluate(train_dataset)\n",
        "val_loss, val_accuracy = discriminator.evaluate(valid_dataset)\n",
        "print(f\"Training Accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Training Accuracy: {val_accuracy*100:.2f}%\")\n",
        "\n",
        "def get_predictions_and_labels(model, dataset):\n",
        "  predictions = []\n",
        "  true_labels = []\n",
        "\n",
        "  for images, labels in dataset:\n",
        "    preds = model.predict(images)\n",
        "    preds = (preds > 0.5).astype(np.int) #convert probabilities to binary labels\n",
        "    predictions.extend(preds.flatten())\n",
        "    true_labels.extend(labels.numpy().flatten())\n",
        "\n",
        "  return np.array(predictions), np.array(true_labels)\n",
        "\n",
        "predictions, true_labels = get_predictions_and_labels(discriminator, valid_dataset)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
        "print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}\")\n",
        "\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "def predict_deepfake(image_path, model):\n",
        "  img = image.load_img(image_path, target_size=(256, 256))\n",
        "  img_array = image.img_to_array(img)\n",
        "  img_array = np.expand_dims(img_array, axis=0) #expand dimensions\n",
        "  img_array = img_array / 255.0 #normalize from 0 to 1\n",
        "  logits = model.predict(img_array)\n",
        "  probabilities = tf.sigmoid(logits) #convert logits to probabilities\n",
        "\n",
        "  return \"fake\" if probabilities.numpy()[0] < 0.5 else \"real\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMQxGyETuUQR"
      },
      "outputs": [],
      "source": [
        "saved_discriminator = load_model('final_discriminator_model.h5')\n",
        "\n",
        "# example usage\n",
        "# result = predict_deepfake('image_path.jpg', saved_discriminator)\n",
        "# print(\"The image is: \", result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}