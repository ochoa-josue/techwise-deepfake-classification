{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ochoa-josue/techwise-deepfake-classification/blob/feature%2Fgan-model/gan_discriminator_deepfake_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HQOv309EpBkn"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet tensorflow matplotlib tensorflow-datasets ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9HkbfG52pMPq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory #load image datasets\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, load_model #sequential api for generator and discriminator\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling2D, BatchNormalization\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.metrics import AUC #area under the curve for model performance (especially imbalanced datasets)\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pma4qS78nxM",
        "outputId": "71cf5197-324d-488c-f3a2-c8798d861d26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWfFq3ojsVyn"
      },
      "source": [
        "### **Retrieve and Preprocess Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNGHH8DHpaWi",
        "outputId": "ec49ffd3-5ca9-42c5-b550-c284b75285d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extracted files: ['valid', 'test', 'train']\n",
            "['valid', 'test', 'train']\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "import re\n",
        "\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "  drive.mount('/content/drive')\n",
        "# os.chdir(\"/content/drive/My Drive/conforming-images-subset\") #change current working directory to specified directory\n",
        "\n",
        "zip_file_path = '/content/drive/My Drive/conforming_images_subset.zip'\n",
        "extracted_dir_path = '/content/conforming-images-subset/'\n",
        "os.makedirs(extracted_dir_path, exist_ok=True) #creates dir if non-existent\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "  zip_ref.extractall(extracted_dir_path)\n",
        "\n",
        "extracted_files = os.listdir(extracted_dir_path)\n",
        "print(f\"extracted files: {extracted_files}\")\n",
        "print(extracted_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "M938AyIzqbP5"
      },
      "outputs": [],
      "source": [
        "# train_dir = \"/content/drive/My Drive/conforming-images-subset/train\"\n",
        "# valid_dir = \"/content/drive/My Drive/conforming-images-subset/valid\"\n",
        "# test_dir = \"/content/drive/My Drive/conforming-images-subset/test\"\n",
        "\n",
        "train_dir = os.path.join(extracted_dir_path, 'train')\n",
        "valid_dir = os.path.join(extracted_dir_path, 'valid')\n",
        "test_dir = os.path.join(extracted_dir_path, 'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "g-dUuF9ptZnm"
      },
      "outputs": [],
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.05),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomZoom(0.05),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomTranslation(height_factor=0.05, width_factor=0.05),\n",
        "])\n",
        "\n",
        "def preprocess(images, labels):\n",
        "  images = tf.cast(images, tf.float32) #convert tensor img to tf.float32 type\n",
        "  images = images / 255.0 #normalize data between 0 to 1\n",
        "\n",
        "  return images, labels\n",
        "\n",
        "def preprocess_with_augmentation(images, labels):\n",
        "  images = data_augmentation(images, training=True)\n",
        "  images = tf.cast(images, tf.float32) #convert tensor img to tf.float32 type\n",
        "  images = images / 255.0 #normalize data between range 0 to 1\n",
        "  return images, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLbtXDiayObL",
        "outputId": "2c517aa8-2e30-40bb-9262-5dc78ae9e842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_dataset = image_dataset_from_directory(train_dir, label_mode='binary', image_size=(256,256), batch_size=32) #image anomaly detection\n",
        "valid_dataset = image_dataset_from_directory(valid_dir, label_mode='binary', image_size=(256,256), batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(test_dir, label_mode='binary', image_size=(256,256), batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mCjfn1vysSC2"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.map(preprocess_with_augmentation)\n",
        "valid_dataset = valid_dataset.map(preprocess)\n",
        "test_dataset = test_dataset.map(preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eBUHqtV6Qe1"
      },
      "source": [
        "### **Build GAN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Z2qn1BlwvEay"
      },
      "outputs": [],
      "source": [
        "class GAN(tf.keras.Model):\n",
        "  def __init__(self, generator, discriminator):\n",
        "    super(GAN, self).__init__() #invoking constructor of tf.keras.Model to initialize GAN proxy object\n",
        "    self.generator = generator\n",
        "    self.discriminator = discriminator\n",
        "\n",
        "  def compile(self, g_optimizer, d_optimizer, loss_function):\n",
        "    super(GAN, self).compile() #ensure compilation of parent class is executed\n",
        "    self.g_optimizer = g_optimizer\n",
        "    self.d_optimizer = d_optimizer\n",
        "    self.loss_function = loss_function\n",
        "\n",
        "  def train_step(self, real_images): #override parent train_step when calling to TensorFlow fit function\n",
        "    batch_size = tf.shape(real_images)[0] #retrieving batch size\n",
        "    noise_dim = 128 #128 dimensions\n",
        "\n",
        "    #generate fake images\n",
        "    random_noise = tf.random.normal(shape=(batch_size, noise_dim)) #create random noise vectors with Gaussian distribution\n",
        "    fake_images = self.generator(random_noise, training=True)\n",
        "\n",
        "    #train discriminator\n",
        "    with tf.GradientTape() as disc_tape: #records operations on tensors; thereafter performs automatic differentiation, in other words, compute derivatives or gradients of functions\n",
        "        real_output = self.discriminator(real_images, training=True)\n",
        "        fake_output = self.discriminator(fake_images, training=True)\n",
        "\n",
        "        #calculate loss\n",
        "        real_loss = self.loss_function(tf.ones_like(real_output), real_output)\n",
        "        fake_loss = self.loss_function(tf.zeros_like(fake_output), fake_output)\n",
        "\n",
        "        #total discriminator loss\n",
        "        disc_loss = real_loss + fake_loss\n",
        "\n",
        "    #calculate gradients for discriminator\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
        "\n",
        "    #apply gradients to the optimizer\n",
        "    self.d_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
        "\n",
        "    #train generator\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "      fake_images = self.generator(random_noise, training=True) #generate fake images\n",
        "      fake_output = self.discriminator(fake_images, training=True) #disc predict fake images\n",
        "      gen_loss = self.loss_function(tf.ones_like(fake_output), fake_output) #show generator loss; maximimize loss to optimize generator\n",
        "\n",
        "    #calculate gradients for generator\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
        "    #apply gradients to the optimizer\n",
        "    self.g_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
        "\n",
        "    return {'disc_loss': disc_loss, 'gen_loss': gen_loss}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "TdePdiVH6sul"
      },
      "outputs": [],
      "source": [
        "def build_generator():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(7*7*128, input_dim=128)) #intakes 128-dimesional input of random noise to output tensor shape\n",
        "  model.add(LeakyReLU(0.2)) #introduce non-linearlity for continued learning\n",
        "  model.add(BatchNormalization(momentum=0.8)) #stabilize training process by tracking moving averages overtime; faster convergence\n",
        "  model.add(Reshape((7, 7, 128))) #reshapes into 4D tensor which inclues batch size, height, width, and channels\n",
        "\n",
        "  #upsampling block 1\n",
        "  model.add(UpSampling2D()) #double dimensions of the input tensor for higher resolutions from low-dimensional noise vectors\n",
        "  model.add(Conv2D(128, 5, padding='same')) #adds a convolutional layer with 256 filters, 5x5 kernel size, and same padding to preserve spatial dimensions as input image\n",
        "  model.add(LeakyReLU(0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  #upsampling block 2\n",
        "  model.add(UpSampling2D())\n",
        "  model.add(Conv2D(64, 5, padding='same')) #reduce filters to prevent overfitting and allow feature abstraction e.g. capturing simple patterns (edges, textures)\n",
        "  model.add(LeakyReLU(0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  #simplify model\n",
        "  #convolution block 1\n",
        "  model.add(Conv2D(128, 4, padding='same'))\n",
        "  model.add(LeakyReLU(0.2))\n",
        "\n",
        "  #output layer adjustment; output layer is tanh activation\n",
        "  model.add(Conv2D(3, 4, padding='same', activation='sigmoid')) #sigmoid activation for data normalized between 0 and 1 for RGB images\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Y53-_E657oGG"
      },
      "outputs": [],
      "source": [
        "def build_discriminator(input_shape=(256, 256, 3)):\n",
        "  model = Sequential([\n",
        "      #convolution block 1\n",
        "      Conv2D(32, kernel_size=5, strides=2, input_shape=input_shape, padding=\"same\", kernel_regularizer=l2(0.01)), #stride reduces spatial dimensions by half\n",
        "      LeakyReLU(alpha=0.2),\n",
        "      Dropout(0.3), #prevent overfitting where 30% of input units or features are randomly set to zero\n",
        "\n",
        "      #convolution block 2\n",
        "      Conv2D(64, kernel_size=5, strides=2, padding=\"same\", kernel_regularizer=l2(0.01)),\n",
        "      LeakyReLU(alpha=0.2),\n",
        "      Dropout(0.3),\n",
        "\n",
        "      #simplify model\n",
        "      #convolution block 3\n",
        "      Conv2D(256, kernel_size=5, strides=2, padding=\"same\"),\n",
        "      LeakyReLU(alpha=0.2),\n",
        "      Dropout(0.3),\n",
        "\n",
        "      #flatten and output layer\n",
        "      Flatten(), #flatten into a 1D tensor for easier integration with fully connected layers\n",
        "      Dense(1, kernel_regularizer=l2(0.01)) #indicate whether image is real or fake\n",
        "  ])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf9CmNVbvyyW"
      },
      "source": [
        "### **Data Visualization and Compiling GAN Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "g7rUHL8N6Sj9"
      },
      "outputs": [],
      "source": [
        "# generator = build_generator()\n",
        "# g_optimizer = tf.keras.optimizers.Adam(1e-4) #find best convergence along with optimal learning rate\n",
        "# noise_dim = 128\n",
        "# gan_model = GAN(generator, discriminator, noise_dim) #create GAN\n",
        "# gan_model.compile(g_optimizer=g_optimizer, d_optimizer=d_optimizer) #compile GAN\n",
        "# gan_model.fit(train_dataset, epochs=50, validation_data=valid_dataset, callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "d_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "discriminator.compile(optimizer=d_optimizer, loss=loss_function, metrics=['accuracy', AUC(name='auc')])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def check_and_remount_drive():\n",
        "#   drive_mount_point = '/content/drive'\n",
        "#   drive_base_path = '/content/drive/My Drive'\n",
        "\n",
        "#   if not os.path.isdir(drive_base_path): #look for base path to ensure drive mounted\n",
        "#     print('Mounting Google Drive')\n",
        "#     drive.mount(drive_mount_point, force_remount=True)\n",
        "#     print(\"Mounted Google Drive successfully\")"
      ],
      "metadata": {
        "id": "rItAOyDeqOgd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from google.colab import output\n",
        "import time\n",
        "import json\n",
        "\n",
        "model_dir = '/content/drive/My Drive/models'\n",
        "log_dir = '/content/drive/My Drive/logs'\n",
        "state_file = os.path.join(model_dir, 'training_state.json')\n",
        "\n",
        "#if dir non-existent\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_file_path = os.path.join(log_dir, f\"training_{current_time}.log.txt\")\n",
        "logging.basicConfig(filename=log_file_path, level=logging.INFO, format='%(asctime)s %(message)s')"
      ],
      "metadata": {
        "id": "UNNA2uCLmajy"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_cb = ModelCheckpoint(os.path.join(model_dir, \"ckpt_epoch_{epoch:02d}.h5\"), save_freq='epoch', verbose=1) #display progress bar during saving\n",
        "# checkpoint_cb = ModelCheckpoint(\"/content/drive/My Drive/models/best_discriminator.h5\", save_best_only=True, monitor='val_loss') #callback to save the best model checkpoint during training\n",
        "early_stopping_cb = EarlyStopping(patience=15, restore_best_weights=True, monitor='val_loss') #stop training if no improvement at 15 consecutive epochs\n",
        "# tensorboard_cb = TensorBoard(log_dir='/content/drive/My Drive/logs', histogram_freq=1) #TensorBoard callback to visualize model performance\n",
        "tensorboard_cb = TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "metadata": {
        "id": "M_Q3DVvQeO0z"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_training_state(epoch, accuracy):\n",
        "  state = {'last_epoch': epoch, 'last_accuracy': accuracy}\n",
        "  with open(state_file, 'w') as f:\n",
        "    json.dump(state, f) #serialize Python objects into JSON formatted str\n",
        "\n",
        "def load_training_state():\n",
        "  if os.path.exists(state_file):\n",
        "    print(f\"Found training state file: {state_file}\")\n",
        "    with open(state_file) as f:\n",
        "      return json.load(f)\n",
        "  else:\n",
        "    print(f\"No training state file found at {state_file}. Creating a new one.\")\n",
        "    save_training_state(0, 0.0)\n",
        "    return {'last_epoch': 0, 'last_accuracy': 0}\n",
        "\n",
        "class SaveStateCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    save_training_state(epoch + 1, logs.get('accuracy', 0.0))\n",
        "\n",
        "state = load_training_state()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RU1w-9OXK8F",
        "outputId": "ffcd562d-058f-4c80-f61f-fd7d20dc2871"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found training state file: /content/drive/My Drive/models/training_state.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_files = [f for f in os.listdir(model_dir) if 'ckpt_epoch_' in f and f.endswith('.h5')] #find latest checkpoint file\n",
        "latest_epoch = state['last_epoch']\n",
        "latest_accuracy = state['last_accuracy']\n",
        "\n",
        "if checkpoint_files: #find max checkpoint\n",
        "    epochs = [int(re.search(r'ckpt_epoch_(\\d+).h5', f).group(1)) for f in checkpoint_files]\n",
        "    latest_epoch = max(epochs)\n",
        "    latest_checkpoint = f'ckpt_epoch_{latest_epoch}.h5'\n",
        "    checkpoint_path = os.path.join(model_dir, latest_checkpoint)\n",
        "\n",
        "    print(f\"Loading weights from {checkpoint_path}\")\n",
        "    discriminator.load_weights(checkpoint_path)\n",
        "    print(f\"Resuming training from epoch {latest_epoch + 1} with last accuracy {latest_accuracy}\")\n",
        "else:\n",
        "    print(\"No checkpoint files found. Starting or continuing from the last known state.\")\n",
        "\n",
        "initial_epoch = latest_epoch\n",
        "total_epochs = 100\n",
        "\n",
        "save_state_cb = SaveStateCallback()\n",
        "\n",
        "history = discriminator.fit(train_dataset, epochs=total_epochs, validation_data=valid_dataset,\n",
        "                  callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb, save_state_cb],\n",
        "                  initial_epoch=initial_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VB9PN0zzmjy0",
        "outputId": "3bb79635-4844-499a-9312-b2bc46f63233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading weights from /content/drive/My Drive/models/ckpt_epoch_47.h5\n",
            "Resuming training from epoch 48 with last accuracy 0.7107999920845032\n",
            "Epoch 48/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5926 - accuracy: 0.7096 - auc: 0.7650\n",
            "Epoch 48: saving model to /content/drive/My Drive/models/ckpt_epoch_48.h5\n",
            "625/625 [==============================] - 3089s 5s/step - loss: 0.5926 - accuracy: 0.7096 - auc: 0.7650 - val_loss: 0.5643 - val_accuracy: 0.7218 - val_auc: 0.7754\n",
            "Epoch 49/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5892 - accuracy: 0.7150 - auc: 0.7692\n",
            "Epoch 49: saving model to /content/drive/My Drive/models/ckpt_epoch_49.h5\n",
            "625/625 [==============================] - 3025s 5s/step - loss: 0.5892 - accuracy: 0.7150 - auc: 0.7692 - val_loss: 0.5815 - val_accuracy: 0.6967 - val_auc: 0.7516\n",
            "Epoch 50/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5904 - accuracy: 0.7076 - auc: 0.7618\n",
            "Epoch 50: saving model to /content/drive/My Drive/models/ckpt_epoch_50.h5\n",
            "625/625 [==============================] - 3019s 5s/step - loss: 0.5904 - accuracy: 0.7076 - auc: 0.7618 - val_loss: 0.5627 - val_accuracy: 0.7247 - val_auc: 0.7803\n",
            "Epoch 51/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5900 - accuracy: 0.7125 - auc: 0.7659\n",
            "Epoch 51: saving model to /content/drive/My Drive/models/ckpt_epoch_51.h5\n",
            "625/625 [==============================] - 3013s 5s/step - loss: 0.5900 - accuracy: 0.7125 - auc: 0.7659 - val_loss: 0.5682 - val_accuracy: 0.7082 - val_auc: 0.7688\n",
            "Epoch 52/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5858 - accuracy: 0.7165 - auc: 0.7698\n",
            "Epoch 52: saving model to /content/drive/My Drive/models/ckpt_epoch_52.h5\n",
            "625/625 [==============================] - 3013s 5s/step - loss: 0.5858 - accuracy: 0.7165 - auc: 0.7698 - val_loss: 0.6197 - val_accuracy: 0.6562 - val_auc: 0.7170\n",
            "Epoch 53/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5879 - accuracy: 0.7179 - auc: 0.7683\n",
            "Epoch 53: saving model to /content/drive/My Drive/models/ckpt_epoch_53.h5\n",
            "625/625 [==============================] - 3001s 5s/step - loss: 0.5879 - accuracy: 0.7179 - auc: 0.7683 - val_loss: 0.6249 - val_accuracy: 0.6463 - val_auc: 0.7098\n",
            "Epoch 54/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5880 - accuracy: 0.7175 - auc: 0.7692\n",
            "Epoch 54: saving model to /content/drive/My Drive/models/ckpt_epoch_54.h5\n",
            "625/625 [==============================] - 2998s 5s/step - loss: 0.5880 - accuracy: 0.7175 - auc: 0.7692 - val_loss: 0.5927 - val_accuracy: 0.6722 - val_auc: 0.7315\n",
            "Epoch 55/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5858 - accuracy: 0.7186 - auc: 0.7708\n",
            "Epoch 55: saving model to /content/drive/My Drive/models/ckpt_epoch_55.h5\n",
            "625/625 [==============================] - 2998s 5s/step - loss: 0.5858 - accuracy: 0.7186 - auc: 0.7708 - val_loss: 0.5647 - val_accuracy: 0.7185 - val_auc: 0.7802\n",
            "Epoch 56/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5849 - accuracy: 0.7194 - auc: 0.7726\n",
            "Epoch 56: saving model to /content/drive/My Drive/models/ckpt_epoch_56.h5\n",
            "625/625 [==============================] - 3008s 5s/step - loss: 0.5849 - accuracy: 0.7194 - auc: 0.7726 - val_loss: 0.5815 - val_accuracy: 0.6798 - val_auc: 0.7460\n",
            "Epoch 57/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5863 - accuracy: 0.7168 - auc: 0.7717\n",
            "Epoch 57: saving model to /content/drive/My Drive/models/ckpt_epoch_57.h5\n",
            "625/625 [==============================] - 3010s 5s/step - loss: 0.5863 - accuracy: 0.7168 - auc: 0.7717 - val_loss: 0.5710 - val_accuracy: 0.7028 - val_auc: 0.7664\n",
            "Epoch 58/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5815 - accuracy: 0.7206 - auc: 0.7725\n",
            "Epoch 58: saving model to /content/drive/My Drive/models/ckpt_epoch_58.h5\n",
            "625/625 [==============================] - 3002s 5s/step - loss: 0.5815 - accuracy: 0.7206 - auc: 0.7725 - val_loss: 0.5736 - val_accuracy: 0.7020 - val_auc: 0.7562\n",
            "Epoch 59/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5833 - accuracy: 0.7225 - auc: 0.7724\n",
            "Epoch 59: saving model to /content/drive/My Drive/models/ckpt_epoch_59.h5\n",
            "625/625 [==============================] - 2992s 5s/step - loss: 0.5833 - accuracy: 0.7225 - auc: 0.7724 - val_loss: 0.5708 - val_accuracy: 0.7005 - val_auc: 0.7604\n",
            "Epoch 60/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5815 - accuracy: 0.7211 - auc: 0.7744\n",
            "Epoch 60: saving model to /content/drive/My Drive/models/ckpt_epoch_60.h5\n",
            "625/625 [==============================] - 2981s 5s/step - loss: 0.5815 - accuracy: 0.7211 - auc: 0.7744 - val_loss: 0.5741 - val_accuracy: 0.6923 - val_auc: 0.7524\n",
            "Epoch 61/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5819 - accuracy: 0.7227 - auc: 0.7747\n",
            "Epoch 61: saving model to /content/drive/My Drive/models/ckpt_epoch_61.h5\n",
            "625/625 [==============================] - 2980s 5s/step - loss: 0.5819 - accuracy: 0.7227 - auc: 0.7747 - val_loss: 0.6106 - val_accuracy: 0.6562 - val_auc: 0.7188\n",
            "Epoch 62/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5803 - accuracy: 0.7218 - auc: 0.7759\n",
            "Epoch 62: saving model to /content/drive/My Drive/models/ckpt_epoch_62.h5\n",
            "625/625 [==============================] - 2977s 5s/step - loss: 0.5803 - accuracy: 0.7218 - auc: 0.7759 - val_loss: 0.5778 - val_accuracy: 0.6925 - val_auc: 0.7482\n",
            "Epoch 63/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5780 - accuracy: 0.7265 - auc: 0.7811\n",
            "Epoch 63: saving model to /content/drive/My Drive/models/ckpt_epoch_63.h5\n",
            "625/625 [==============================] - 2962s 5s/step - loss: 0.5780 - accuracy: 0.7265 - auc: 0.7811 - val_loss: 0.5740 - val_accuracy: 0.6973 - val_auc: 0.7532\n",
            "Epoch 64/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5781 - accuracy: 0.7259 - auc: 0.7798\n",
            "Epoch 64: saving model to /content/drive/My Drive/models/ckpt_epoch_64.h5\n",
            "625/625 [==============================] - 2979s 5s/step - loss: 0.5781 - accuracy: 0.7259 - auc: 0.7798 - val_loss: 0.5733 - val_accuracy: 0.6933 - val_auc: 0.7539\n",
            "Epoch 65/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5806 - accuracy: 0.7225 - auc: 0.7752\n",
            "Epoch 65: saving model to /content/drive/My Drive/models/ckpt_epoch_65.h5\n",
            "625/625 [==============================] - 2980s 5s/step - loss: 0.5806 - accuracy: 0.7225 - auc: 0.7752 - val_loss: 0.5486 - val_accuracy: 0.7405 - val_auc: 0.7957\n",
            "Epoch 66/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5782 - accuracy: 0.7258 - auc: 0.7783\n",
            "Epoch 66: saving model to /content/drive/My Drive/models/ckpt_epoch_66.h5\n",
            "625/625 [==============================] - 2984s 5s/step - loss: 0.5782 - accuracy: 0.7258 - auc: 0.7783 - val_loss: 0.5664 - val_accuracy: 0.7078 - val_auc: 0.7640\n",
            "Epoch 67/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5777 - accuracy: 0.7275 - auc: 0.7777\n",
            "Epoch 67: saving model to /content/drive/My Drive/models/ckpt_epoch_67.h5\n",
            "625/625 [==============================] - 3002s 5s/step - loss: 0.5777 - accuracy: 0.7275 - auc: 0.7777 - val_loss: 0.5782 - val_accuracy: 0.6902 - val_auc: 0.7501\n",
            "Epoch 68/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5756 - accuracy: 0.7275 - auc: 0.7795\n",
            "Epoch 68: saving model to /content/drive/My Drive/models/ckpt_epoch_68.h5\n",
            "625/625 [==============================] - 2985s 5s/step - loss: 0.5756 - accuracy: 0.7275 - auc: 0.7795 - val_loss: 0.5604 - val_accuracy: 0.7212 - val_auc: 0.7802\n",
            "Epoch 69/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5757 - accuracy: 0.7287 - auc: 0.7808\n",
            "Epoch 69: saving model to /content/drive/My Drive/models/ckpt_epoch_69.h5\n",
            "625/625 [==============================] - 2982s 5s/step - loss: 0.5757 - accuracy: 0.7287 - auc: 0.7808 - val_loss: 0.5740 - val_accuracy: 0.6957 - val_auc: 0.7553\n",
            "Epoch 70/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5776 - accuracy: 0.7293 - auc: 0.7784\n",
            "Epoch 70: saving model to /content/drive/My Drive/models/ckpt_epoch_70.h5\n",
            "625/625 [==============================] - 2985s 5s/step - loss: 0.5776 - accuracy: 0.7293 - auc: 0.7784 - val_loss: 0.5618 - val_accuracy: 0.7130 - val_auc: 0.7746\n",
            "Epoch 71/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5781 - accuracy: 0.7266 - auc: 0.7801\n",
            "Epoch 71: saving model to /content/drive/My Drive/models/ckpt_epoch_71.h5\n",
            "625/625 [==============================] - 2975s 5s/step - loss: 0.5781 - accuracy: 0.7266 - auc: 0.7801 - val_loss: 0.5805 - val_accuracy: 0.6862 - val_auc: 0.7484\n",
            "Epoch 72/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5725 - accuracy: 0.7303 - auc: 0.7818\n",
            "Epoch 72: saving model to /content/drive/My Drive/models/ckpt_epoch_72.h5\n",
            "625/625 [==============================] - 2963s 5s/step - loss: 0.5725 - accuracy: 0.7303 - auc: 0.7818 - val_loss: 0.5594 - val_accuracy: 0.7197 - val_auc: 0.7725\n",
            "Epoch 73/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5755 - accuracy: 0.7298 - auc: 0.7832\n",
            "Epoch 73: saving model to /content/drive/My Drive/models/ckpt_epoch_73.h5\n",
            "625/625 [==============================] - 2974s 5s/step - loss: 0.5755 - accuracy: 0.7298 - auc: 0.7832 - val_loss: 0.5504 - val_accuracy: 0.7262 - val_auc: 0.7815\n",
            "Epoch 74/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5745 - accuracy: 0.7286 - auc: 0.7799\n",
            "Epoch 74: saving model to /content/drive/My Drive/models/ckpt_epoch_74.h5\n",
            "625/625 [==============================] - 2989s 5s/step - loss: 0.5745 - accuracy: 0.7286 - auc: 0.7799 - val_loss: 0.5554 - val_accuracy: 0.7498 - val_auc: 0.8002\n",
            "Epoch 75/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5750 - accuracy: 0.7282 - auc: 0.7819\n",
            "Epoch 75: saving model to /content/drive/My Drive/models/ckpt_epoch_75.h5\n",
            "625/625 [==============================] - 2976s 5s/step - loss: 0.5750 - accuracy: 0.7282 - auc: 0.7819 - val_loss: 0.5595 - val_accuracy: 0.7125 - val_auc: 0.7685\n",
            "Epoch 76/100\n",
            "415/625 [==================>...........] - ETA: 15:53 - loss: 0.5746 - accuracy: 0.7289 - auc: 0.7829"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# state = load_training_state()\n",
        "# start_epoch = state['last_epoch']\n",
        "# total_epochs = 100\n",
        "# # epochs_per_batch = 10\n",
        "\n",
        "# output.clear()\n",
        "# print(\"training start\")\n",
        "\n",
        "# latest_checkpoint = tf.train.latest_checkpoint(model_dir)\n",
        "# if latest_checkpoint:\n",
        "#   logging.info(f\"Resuming from {latest_checkpoint}, at accuracy {state['last_accuracy']}\")\n",
        "#   discriminator.load_weights(latest_checkpoint)\n",
        "\n",
        "# save_state_cb = SaveStateCallback()\n",
        "\n",
        "# discriminator.fit(train_dataset, epochs=total_epochs, validation_data=valid_dataset, callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb], initial_epoch=start_epoch)\n",
        "\n",
        "#----\n",
        "\n",
        "# check_and_remount_drive() #ensure Drive is mounted before logging\n",
        "\n",
        "# for batch in range((start_epoch - 1) // epochs_per_batch, total_epochs // epochs_per_batch):\n",
        "#   latest_checkpoint = tf.train.latest_checkpoint(model_dir)\n",
        "#   if latest_checkpoint:\n",
        "#     logging.info(f\"Resuming from {latest_checkpoint}, at accuracy {state['last_accuracy']}\")\n",
        "#     discriminator.load_weights(latest_checkpoint)\n",
        "\n",
        "#   # start_batch_epoch = max(start_epoch - batch * epochs_per_batch, 1)\n",
        "#   # end_batch_epoch = start_batch_epoch + epochs_per_batch\n",
        "#   end_epoch = min((batch+1) * epochs_per_batch, total_epochs) #doesn't exceed total_epochs\n",
        "#   for epoch in range(start_epoch, end_epoch + 1): # add 1 to include end_epoch\n",
        "#     history = discriminator.fit(train_dataset, epochs=1, validation_data=valid_dataset, callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb], initial_epoch=epoch-1, epochs=epoch) #epoch-1 to for 0 indexing in TensorFlow\n",
        "\n",
        "#     current_accuracy = history.history['accuracy'][-1] if 'accuracy' in history.history else 0\n",
        "#     save_training_state(batch+1, epoch, current_accuracy)\n",
        "\n",
        "#     logging.info(f\"resumed at accuracy {current_accuracy}, batch {batch+1}, epoch {epoch}/{total_epochs}: loss={history.history['loss'][-1]}, accuracy={history.history['accuracy'][-1]}, val_loss={history.history['val_loss'][-1]}, val_accuracy={history.history.get('val_accuracy', [-1])[-1]}\")\n",
        "\n",
        "#     if epoch % epochs_per_batch == 0 or epoch == total_epochs:\n",
        "#       output.clear()\n",
        "\n",
        "#   start_epoch = end_epoch + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "q8HPoQStd8G3",
        "outputId": "7c7be321-b69d-4b4c-dfeb-cc33b949efc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training start\n",
            "Epoch 1/100\n",
            "  2/625 [..............................] - ETA: 48:23 - loss: 1.2264 - accuracy: 0.4688 - auc: 0.4390"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-524669807823>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0msave_state_cb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveStateCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_cb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_cb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_cb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# check_and_remount_drive() #ensure Drive is mounted before logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = discriminator.evaluate(test_dataset) #evaluate model performance for unseen data\n",
        "logging.info(f\"test accuracy: {test_accuracy*100:.2f}%\")\n",
        "print(f\"test accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "p-D51KsPfBFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator.save('/content/drive/My Drive/final_discriminator_model.h5')"
      ],
      "metadata": {
        "id": "fo3DdywlfBuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YU2FXT3xpDyY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "#visualize AUC\n",
        "plt.plot(history.history['auc'], label='Training AUC')\n",
        "plt.plot(history.history['val_auc'], label='Validation AUC')\n",
        "plt.title('Training and Validation AUC')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize training process\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6ievPAaDk2Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate model performance\n",
        "train_loss, train_accuracy = discriminator.evaluate(train_dataset)\n",
        "val_loss, val_accuracy = discriminator.evaluate(valid_dataset)\n",
        "print(f\"Training Accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Training Accuracy: {val_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "EdbRVvXhk47a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions_and_labels(model, dataset):\n",
        "  predictions = []\n",
        "  true_labels = []\n",
        "\n",
        "  for images, labels in dataset:\n",
        "    preds = model.predict(images)\n",
        "    preds = (preds > 0.5).astype(np.int) #convert probabilities to binary labels\n",
        "    predictions.extend(preds.flatten())\n",
        "    true_labels.extend(labels.numpy().flatten())\n",
        "\n",
        "  return np.array(predictions), np.array(true_labels)"
      ],
      "metadata": {
        "id": "YgCya5gHk8i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, true_labels = get_predictions_and_labels(discriminator, valid_dataset)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
        "print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "GeFwGVuFk-d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(true_labels, predictions)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Sh-XCshTlA5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_deepfake(image_path, model):\n",
        "  img = image.load_img(image_path, target_size=(256, 256))\n",
        "  img_array = image.img_to_array(img)\n",
        "  img_array = np.expand_dims(img_array, axis=0) #expand dimensions\n",
        "  img_array = img_array / 255.0 #normalize from 0 to 1\n",
        "  logits = model.predict(img_array)\n",
        "  probabilities = tf.sigmoid(logits) #convert logits to probabilities\n",
        "\n",
        "  return \"fake\" if probabilities.numpy()[0] < 0.5 else \"real\""
      ],
      "metadata": {
        "id": "0LdQTu3NktN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMQxGyETuUQR"
      },
      "outputs": [],
      "source": [
        "saved_discriminator = load_model('final_discriminator_model.h5')\n",
        "\n",
        "# example usage\n",
        "# result = predict_deepfake('image_path.jpg', saved_discriminator)\n",
        "# print(\"The image is: \", result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}