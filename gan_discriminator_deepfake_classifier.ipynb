{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ochoa-josue/techwise-deepfake-classification/blob/feature%2Fgan-model/gan_discriminator_deepfake_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HQOv309EpBkn"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet tensorflow matplotlib tensorflow-datasets ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9HkbfG52pMPq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory #load image datasets\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, load_model #sequential api for generator and discriminator\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling2D, BatchNormalization\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.metrics import AUC #area under the curve for model performance (especially imbalanced datasets)\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pma4qS78nxM",
        "outputId": "60fb7b26-264a-47df-ec86-99a4bc87cd36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWfFq3ojsVyn"
      },
      "source": [
        "### **Retrieve and Preprocess Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNGHH8DHpaWi",
        "outputId": "ba55d395-7f2a-4eea-9228-7ca2f6ef44d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "extracted files: ['valid', 'test', 'train']\n",
            "['valid', 'test', 'train']\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "import re\n",
        "\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "  drive.mount('/content/drive')\n",
        "# os.chdir(\"/content/drive/My Drive/conforming-images-subset\") #change current working directory to specified directory\n",
        "\n",
        "zip_file_path = '/content/drive/My Drive/conforming_images_subset.zip'\n",
        "extracted_dir_path = '/content/conforming-images-subset/'\n",
        "os.makedirs(extracted_dir_path, exist_ok=True) #creates dir if non-existent\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "  zip_ref.extractall(extracted_dir_path)\n",
        "\n",
        "extracted_files = os.listdir(extracted_dir_path)\n",
        "print(f\"extracted files: {extracted_files}\")\n",
        "print(extracted_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "M938AyIzqbP5"
      },
      "outputs": [],
      "source": [
        "# train_dir = \"/content/drive/My Drive/conforming-images-subset/train\"\n",
        "# valid_dir = \"/content/drive/My Drive/conforming-images-subset/valid\"\n",
        "# test_dir = \"/content/drive/My Drive/conforming-images-subset/test\"\n",
        "\n",
        "train_dir = os.path.join(extracted_dir_path, 'train')\n",
        "valid_dir = os.path.join(extracted_dir_path, 'valid')\n",
        "test_dir = os.path.join(extracted_dir_path, 'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g-dUuF9ptZnm"
      },
      "outputs": [],
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.05),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomZoom(0.05),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomTranslation(height_factor=0.05, width_factor=0.05),\n",
        "])\n",
        "\n",
        "def preprocess(images, labels):\n",
        "  images = tf.cast(images, tf.float32) #convert tensor img to tf.float32 type\n",
        "  images = images / 255.0 #normalize data between 0 to 1\n",
        "\n",
        "  return images, labels\n",
        "\n",
        "def preprocess_with_augmentation(images, labels):\n",
        "  images = data_augmentation(images, training=True)\n",
        "  images = tf.cast(images, tf.float32) #convert tensor img to tf.float32 type\n",
        "  images = images / 255.0 #normalize data between range 0 to 1\n",
        "  return images, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLbtXDiayObL",
        "outputId": "cea1d903-1c99-4fcb-ec4b-02f0d2b9f519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n",
            "Found 4000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_dataset = image_dataset_from_directory(train_dir, label_mode='binary', image_size=(256,256), batch_size=32) #image anomaly detection\n",
        "valid_dataset = image_dataset_from_directory(valid_dir, label_mode='binary', image_size=(256,256), batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(test_dir, label_mode='binary', image_size=(256,256), batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mCjfn1vysSC2"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.map(preprocess_with_augmentation)\n",
        "valid_dataset = valid_dataset.map(preprocess)\n",
        "test_dataset = test_dataset.map(preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eBUHqtV6Qe1"
      },
      "source": [
        "### **Build GAN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Z2qn1BlwvEay"
      },
      "outputs": [],
      "source": [
        "class GAN(tf.keras.Model):\n",
        "  def __init__(self, generator, discriminator):\n",
        "    super(GAN, self).__init__() #invoking constructor of tf.keras.Model to initialize GAN proxy object\n",
        "    self.generator = generator\n",
        "    self.discriminator = discriminator\n",
        "\n",
        "  def compile(self, g_optimizer, d_optimizer, loss_function):\n",
        "    super(GAN, self).compile() #ensure compilation of parent class is executed\n",
        "    self.g_optimizer = g_optimizer\n",
        "    self.d_optimizer = d_optimizer\n",
        "    self.loss_function = loss_function\n",
        "\n",
        "  def train_step(self, real_images): #override parent train_step when calling to TensorFlow fit function\n",
        "    batch_size = tf.shape(real_images)[0] #retrieving batch size\n",
        "    noise_dim = 128 #128 dimensions\n",
        "\n",
        "    #generate fake images\n",
        "    random_noise = tf.random.normal(shape=(batch_size, noise_dim)) #create random noise vectors with Gaussian distribution\n",
        "    fake_images = self.generator(random_noise, training=True)\n",
        "\n",
        "    #train discriminator\n",
        "    with tf.GradientTape() as disc_tape: #records operations on tensors; thereafter performs automatic differentiation, in other words, compute derivatives or gradients of functions\n",
        "        real_output = self.discriminator(real_images, training=True)\n",
        "        fake_output = self.discriminator(fake_images, training=True)\n",
        "\n",
        "        #calculate loss\n",
        "        real_loss = self.loss_function(tf.ones_like(real_output), real_output)\n",
        "        fake_loss = self.loss_function(tf.zeros_like(fake_output), fake_output)\n",
        "\n",
        "        #total discriminator loss\n",
        "        disc_loss = real_loss + fake_loss\n",
        "\n",
        "    #calculate gradients for discriminator\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
        "\n",
        "    #apply gradients to the optimizer\n",
        "    self.d_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
        "\n",
        "    #train generator\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "      fake_images = self.generator(random_noise, training=True) #generate fake images\n",
        "      fake_output = self.discriminator(fake_images, training=True) #disc predict fake images\n",
        "      gen_loss = self.loss_function(tf.ones_like(fake_output), fake_output) #show generator loss; maximimize loss to optimize generator\n",
        "\n",
        "    #calculate gradients for generator\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
        "    #apply gradients to the optimizer\n",
        "    self.g_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
        "\n",
        "    return {'disc_loss': disc_loss, 'gen_loss': gen_loss}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TdePdiVH6sul"
      },
      "outputs": [],
      "source": [
        "def build_generator():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(7*7*128, input_dim=128)) #intakes 128-dimesional input of random noise to output tensor shape\n",
        "  model.add(LeakyReLU(0.2)) #introduce non-linearlity for continued learning\n",
        "  model.add(BatchNormalization(momentum=0.8)) #stabilize training process by tracking moving averages overtime; faster convergence\n",
        "  model.add(Reshape((7, 7, 128))) #reshapes into 4D tensor which inclues batch size, height, width, and channels\n",
        "\n",
        "  #upsampling block 1\n",
        "  model.add(UpSampling2D()) #double dimensions of the input tensor for higher resolutions from low-dimensional noise vectors\n",
        "  model.add(Conv2D(128, 5, padding='same')) #adds a convolutional layer with 256 filters, 5x5 kernel size, and same padding to preserve spatial dimensions as input image\n",
        "  model.add(LeakyReLU(0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  #upsampling block 2\n",
        "  model.add(UpSampling2D())\n",
        "  model.add(Conv2D(64, 5, padding='same')) #reduce filters to prevent overfitting and allow feature abstraction e.g. capturing simple patterns (edges, textures)\n",
        "  model.add(LeakyReLU(0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  #simplify model\n",
        "  #convolution block 1\n",
        "  model.add(Conv2D(128, 4, padding='same'))\n",
        "  model.add(LeakyReLU(0.2))\n",
        "\n",
        "  #output layer adjustment; output layer is tanh activation\n",
        "  model.add(Conv2D(3, 4, padding='same', activation='sigmoid')) #sigmoid activation for data normalized between 0 and 1 for RGB images\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Y53-_E657oGG"
      },
      "outputs": [],
      "source": [
        "def build_discriminator(input_shape=(256, 256, 3)):\n",
        "  model = Sequential([\n",
        "      #convolution block 1\n",
        "      Conv2D(32, kernel_size=5, strides=2, input_shape=input_shape, padding=\"same\", kernel_regularizer=l2(0.01)), #stride reduces spatial dimensions by half\n",
        "      LeakyReLU(alpha=0.2),\n",
        "      Dropout(0.3), #prevent overfitting where 30% of input units or features are randomly set to zero\n",
        "\n",
        "      #convolution block 2\n",
        "      Conv2D(64, kernel_size=5, strides=2, padding=\"same\", kernel_regularizer=l2(0.01)),\n",
        "      LeakyReLU(alpha=0.2),\n",
        "      Dropout(0.3),\n",
        "\n",
        "      #simplify model\n",
        "      #convolution block 3\n",
        "      Conv2D(256, kernel_size=5, strides=2, padding=\"same\"),\n",
        "      LeakyReLU(alpha=0.2),\n",
        "      Dropout(0.3),\n",
        "\n",
        "      #flatten and output layer\n",
        "      Flatten(), #flatten into a 1D tensor for easier integration with fully connected layers\n",
        "      Dense(1, kernel_regularizer=l2(0.01)) #indicate whether image is real or fake\n",
        "  ])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf9CmNVbvyyW"
      },
      "source": [
        "### **Data Visualization and Compiling GAN Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "g7rUHL8N6Sj9"
      },
      "outputs": [],
      "source": [
        "# generator = build_generator()\n",
        "# g_optimizer = tf.keras.optimizers.Adam(1e-4) #find best convergence along with optimal learning rate\n",
        "# noise_dim = 128\n",
        "# gan_model = GAN(generator, discriminator, noise_dim) #create GAN\n",
        "# gan_model.compile(g_optimizer=g_optimizer, d_optimizer=d_optimizer) #compile GAN\n",
        "# gan_model.fit(train_dataset, epochs=50, validation_data=valid_dataset, callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "d_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "discriminator.compile(optimizer=d_optimizer, loss=loss_function, metrics=['accuracy', AUC(name='auc')])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def check_and_remount_drive():\n",
        "#   drive_mount_point = '/content/drive'\n",
        "#   drive_base_path = '/content/drive/My Drive'\n",
        "\n",
        "#   if not os.path.isdir(drive_base_path): #look for base path to ensure drive mounted\n",
        "#     print('Mounting Google Drive')\n",
        "#     drive.mount(drive_mount_point, force_remount=True)\n",
        "#     print(\"Mounted Google Drive successfully\")"
      ],
      "metadata": {
        "id": "rItAOyDeqOgd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from google.colab import output\n",
        "import time\n",
        "import json\n",
        "\n",
        "model_dir = '/content/drive/My Drive/models'\n",
        "log_dir = '/content/drive/My Drive/logs'\n",
        "state_file = os.path.join(model_dir, 'training_state.json')\n",
        "\n",
        "#if dir non-existent\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_file_path = os.path.join(log_dir, f\"training_{current_time}.log.txt\")\n",
        "logging.basicConfig(filename=log_file_path, level=logging.INFO, format='%(asctime)s %(message)s')"
      ],
      "metadata": {
        "id": "UNNA2uCLmajy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_cb = ModelCheckpoint(os.path.join(model_dir, \"ckpt_epoch_{epoch:02d}.h5\"), save_freq='epoch', verbose=1) #display progress bar during saving\n",
        "# checkpoint_cb = ModelCheckpoint(\"/content/drive/My Drive/models/best_discriminator.h5\", save_best_only=True, monitor='val_loss') #callback to save the best model checkpoint during training\n",
        "early_stopping_cb = EarlyStopping(patience=15, restore_best_weights=True, monitor='val_loss') #stop training if no improvement at 15 consecutive epochs\n",
        "# tensorboard_cb = TensorBoard(log_dir='/content/drive/My Drive/logs', histogram_freq=1) #TensorBoard callback to visualize model performance\n",
        "tensorboard_cb = TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "metadata": {
        "id": "M_Q3DVvQeO0z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_training_state(epoch, accuracy):\n",
        "  state = {'last_epoch': epoch, 'last_accuracy': accuracy}\n",
        "  with open(state_file, 'w') as f:\n",
        "    json.dump(state, f) #serialize Python objects into JSON formatted str\n",
        "\n",
        "def load_training_state():\n",
        "  if os.path.exists(state_file):\n",
        "    print(f\"Found training state file: {state_file}\")\n",
        "    with open(state_file) as f:\n",
        "      return json.load(f)\n",
        "  else:\n",
        "    print(f\"No training state file found at {state_file}. Creating a new one.\")\n",
        "    save_training_state(0, 0.0)\n",
        "    return {'last_epoch': 0, 'last_accuracy': 0}\n",
        "\n",
        "class SaveStateCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    save_training_state(epoch + 1, logs.get('accuracy', 0.0))\n",
        "\n",
        "state = load_training_state()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RU1w-9OXK8F",
        "outputId": "a45ab77a-ac32-49ae-b7d4-668267543600"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found training state file: /content/drive/My Drive/models/training_state.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_files = [f for f in os.listdir(model_dir) if 'ckpt_epoch_' in f and f.endswith('.h5')] #find latest checkpoint file\n",
        "latest_epoch = state['last_epoch']\n",
        "latest_accuracy = state['last_accuracy']\n",
        "\n",
        "if checkpoint_files: #find max checkpoint\n",
        "    epochs = [int(re.search(r'ckpt_epoch_(\\d+).h5', f).group(1)) for f in checkpoint_files]\n",
        "    latest_epoch = max(epochs)\n",
        "    latest_checkpoint = f'ckpt_epoch_{latest_epoch}.h5'\n",
        "    checkpoint_path = os.path.join(model_dir, latest_checkpoint)\n",
        "\n",
        "    print(f\"Loading weights from {checkpoint_path}\")\n",
        "    discriminator.load_weights(checkpoint_path)\n",
        "    print(f\"Resuming training from epoch {latest_epoch + 1} with last accuracy {latest_accuracy}\")\n",
        "else:\n",
        "    print(\"No checkpoint files found. Starting or continuing from the last known state.\")\n",
        "\n",
        "initial_epoch = latest_epoch\n",
        "total_epochs = 100\n",
        "\n",
        "save_state_cb = SaveStateCallback()\n",
        "\n",
        "history = discriminator.fit(train_dataset, epochs=total_epochs, validation_data=valid_dataset,\n",
        "                  callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb, save_state_cb],\n",
        "                  initial_epoch=initial_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VB9PN0zzmjy0",
        "outputId": "7ec7aa9e-48c2-4f0d-d5a9-137e62cc283c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading weights from /content/drive/My Drive/models/ckpt_epoch_75.h5\n",
            "Resuming training from epoch 76 with last accuracy 0.7282000184059143\n",
            "Epoch 76/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5727 - accuracy: 0.7333 - auc: 0.7845\n",
            "Epoch 76: saving model to /content/drive/My Drive/models/ckpt_epoch_76.h5\n",
            "625/625 [==============================] - 3288s 5s/step - loss: 0.5727 - accuracy: 0.7333 - auc: 0.7845 - val_loss: 0.5584 - val_accuracy: 0.7237 - val_auc: 0.7797\n",
            "Epoch 77/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5733 - accuracy: 0.7330 - auc: 0.7831\n",
            "Epoch 77: saving model to /content/drive/My Drive/models/ckpt_epoch_77.h5\n",
            "625/625 [==============================] - 3158s 5s/step - loss: 0.5733 - accuracy: 0.7330 - auc: 0.7831 - val_loss: 0.5688 - val_accuracy: 0.7072 - val_auc: 0.7624\n",
            "Epoch 78/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5697 - accuracy: 0.7340 - auc: 0.7860\n",
            "Epoch 78: saving model to /content/drive/My Drive/models/ckpt_epoch_78.h5\n",
            "625/625 [==============================] - 3196s 5s/step - loss: 0.5697 - accuracy: 0.7340 - auc: 0.7860 - val_loss: 0.5661 - val_accuracy: 0.7025 - val_auc: 0.7592\n",
            "Epoch 79/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5683 - accuracy: 0.7365 - auc: 0.7879\n",
            "Epoch 79: saving model to /content/drive/My Drive/models/ckpt_epoch_79.h5\n",
            "625/625 [==============================] - 3114s 5s/step - loss: 0.5683 - accuracy: 0.7365 - auc: 0.7879 - val_loss: 0.5744 - val_accuracy: 0.6908 - val_auc: 0.7533\n",
            "Epoch 80/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5701 - accuracy: 0.7351 - auc: 0.7850\n",
            "Epoch 80: saving model to /content/drive/My Drive/models/ckpt_epoch_80.h5\n",
            "625/625 [==============================] - 3143s 5s/step - loss: 0.5701 - accuracy: 0.7351 - auc: 0.7850 - val_loss: 0.5456 - val_accuracy: 0.7368 - val_auc: 0.7960\n",
            "Epoch 81/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5717 - accuracy: 0.7346 - auc: 0.7882\n",
            "Epoch 81: saving model to /content/drive/My Drive/models/ckpt_epoch_81.h5\n",
            "625/625 [==============================] - 3307s 5s/step - loss: 0.5717 - accuracy: 0.7346 - auc: 0.7882 - val_loss: 0.5624 - val_accuracy: 0.7092 - val_auc: 0.7700\n",
            "Epoch 82/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5694 - accuracy: 0.7383 - auc: 0.7905\n",
            "Epoch 82: saving model to /content/drive/My Drive/models/ckpt_epoch_82.h5\n",
            "625/625 [==============================] - 3273s 5s/step - loss: 0.5694 - accuracy: 0.7383 - auc: 0.7905 - val_loss: 0.5821 - val_accuracy: 0.6877 - val_auc: 0.7493\n",
            "Epoch 83/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5671 - accuracy: 0.7388 - auc: 0.7882\n",
            "Epoch 83: saving model to /content/drive/My Drive/models/ckpt_epoch_83.h5\n",
            "625/625 [==============================] - 3209s 5s/step - loss: 0.5671 - accuracy: 0.7388 - auc: 0.7882 - val_loss: 0.5636 - val_accuracy: 0.7038 - val_auc: 0.7645\n",
            "Epoch 84/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5667 - accuracy: 0.7386 - auc: 0.7895\n",
            "Epoch 84: saving model to /content/drive/My Drive/models/ckpt_epoch_84.h5\n",
            "625/625 [==============================] - 3236s 5s/step - loss: 0.5667 - accuracy: 0.7386 - auc: 0.7895 - val_loss: 0.5765 - val_accuracy: 0.6920 - val_auc: 0.7537\n",
            "Epoch 85/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5679 - accuracy: 0.7367 - auc: 0.7882\n",
            "Epoch 85: saving model to /content/drive/My Drive/models/ckpt_epoch_85.h5\n",
            "625/625 [==============================] - 3278s 5s/step - loss: 0.5679 - accuracy: 0.7367 - auc: 0.7882 - val_loss: 0.5576 - val_accuracy: 0.7103 - val_auc: 0.7719\n",
            "Epoch 86/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5671 - accuracy: 0.7387 - auc: 0.7881\n",
            "Epoch 86: saving model to /content/drive/My Drive/models/ckpt_epoch_86.h5\n",
            "625/625 [==============================] - 3277s 5s/step - loss: 0.5671 - accuracy: 0.7387 - auc: 0.7881 - val_loss: 0.5386 - val_accuracy: 0.7340 - val_auc: 0.7900\n",
            "Epoch 87/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5629 - accuracy: 0.7436 - auc: 0.7936\n",
            "Epoch 87: saving model to /content/drive/My Drive/models/ckpt_epoch_87.h5\n",
            "625/625 [==============================] - 3294s 5s/step - loss: 0.5629 - accuracy: 0.7436 - auc: 0.7936 - val_loss: 0.5668 - val_accuracy: 0.7095 - val_auc: 0.7680\n",
            "Epoch 88/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5645 - accuracy: 0.7383 - auc: 0.7925\n",
            "Epoch 88: saving model to /content/drive/My Drive/models/ckpt_epoch_88.h5\n",
            "625/625 [==============================] - 3235s 5s/step - loss: 0.5645 - accuracy: 0.7383 - auc: 0.7925 - val_loss: 0.5466 - val_accuracy: 0.7322 - val_auc: 0.7870\n",
            "Epoch 89/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5636 - accuracy: 0.7399 - auc: 0.7927\n",
            "Epoch 89: saving model to /content/drive/My Drive/models/ckpt_epoch_89.h5\n",
            "625/625 [==============================] - 3223s 5s/step - loss: 0.5636 - accuracy: 0.7399 - auc: 0.7927 - val_loss: 0.5563 - val_accuracy: 0.7318 - val_auc: 0.7838\n",
            "Epoch 90/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5617 - accuracy: 0.7412 - auc: 0.7932\n",
            "Epoch 90: saving model to /content/drive/My Drive/models/ckpt_epoch_90.h5\n",
            "625/625 [==============================] - 3313s 5s/step - loss: 0.5617 - accuracy: 0.7412 - auc: 0.7932 - val_loss: 0.5497 - val_accuracy: 0.7278 - val_auc: 0.7847\n",
            "Epoch 91/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5657 - accuracy: 0.7418 - auc: 0.7918\n",
            "Epoch 91: saving model to /content/drive/My Drive/models/ckpt_epoch_91.h5\n",
            "625/625 [==============================] - 3235s 5s/step - loss: 0.5657 - accuracy: 0.7418 - auc: 0.7918 - val_loss: 0.5444 - val_accuracy: 0.7558 - val_auc: 0.8088\n",
            "Epoch 92/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5664 - accuracy: 0.7401 - auc: 0.7930\n",
            "Epoch 92: saving model to /content/drive/My Drive/models/ckpt_epoch_92.h5\n",
            "625/625 [==============================] - 3247s 5s/step - loss: 0.5664 - accuracy: 0.7401 - auc: 0.7930 - val_loss: 0.5455 - val_accuracy: 0.7362 - val_auc: 0.7912\n",
            "Epoch 93/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5624 - accuracy: 0.7393 - auc: 0.7931\n",
            "Epoch 93: saving model to /content/drive/My Drive/models/ckpt_epoch_93.h5\n",
            "625/625 [==============================] - 3290s 5s/step - loss: 0.5624 - accuracy: 0.7393 - auc: 0.7931 - val_loss: 0.5357 - val_accuracy: 0.7380 - val_auc: 0.7945\n",
            "Epoch 94/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5628 - accuracy: 0.7415 - auc: 0.7938\n",
            "Epoch 94: saving model to /content/drive/My Drive/models/ckpt_epoch_94.h5\n",
            "625/625 [==============================] - 3318s 5s/step - loss: 0.5628 - accuracy: 0.7415 - auc: 0.7938 - val_loss: 0.5583 - val_accuracy: 0.7105 - val_auc: 0.7730\n",
            "Epoch 95/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5624 - accuracy: 0.7437 - auc: 0.7934\n",
            "Epoch 95: saving model to /content/drive/My Drive/models/ckpt_epoch_95.h5\n",
            "625/625 [==============================] - 3314s 5s/step - loss: 0.5624 - accuracy: 0.7437 - auc: 0.7934 - val_loss: 0.5347 - val_accuracy: 0.7492 - val_auc: 0.8079\n",
            "Epoch 96/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5601 - accuracy: 0.7461 - auc: 0.7950\n",
            "Epoch 96: saving model to /content/drive/My Drive/models/ckpt_epoch_96.h5\n",
            "625/625 [==============================] - 3321s 5s/step - loss: 0.5601 - accuracy: 0.7461 - auc: 0.7950 - val_loss: 0.5540 - val_accuracy: 0.7812 - val_auc: 0.8239\n",
            "Epoch 97/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5609 - accuracy: 0.7473 - auc: 0.7963\n",
            "Epoch 97: saving model to /content/drive/My Drive/models/ckpt_epoch_97.h5\n",
            "625/625 [==============================] - 3247s 5s/step - loss: 0.5609 - accuracy: 0.7473 - auc: 0.7963 - val_loss: 0.5694 - val_accuracy: 0.6952 - val_auc: 0.7582\n",
            "Epoch 98/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5616 - accuracy: 0.7436 - auc: 0.7942\n",
            "Epoch 98: saving model to /content/drive/My Drive/models/ckpt_epoch_98.h5\n",
            "625/625 [==============================] - 3309s 5s/step - loss: 0.5616 - accuracy: 0.7436 - auc: 0.7942 - val_loss: 0.5439 - val_accuracy: 0.7387 - val_auc: 0.7920\n",
            "Epoch 99/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5595 - accuracy: 0.7472 - auc: 0.7951\n",
            "Epoch 99: saving model to /content/drive/My Drive/models/ckpt_epoch_99.h5\n",
            "625/625 [==============================] - 3281s 5s/step - loss: 0.5595 - accuracy: 0.7472 - auc: 0.7951 - val_loss: 0.5603 - val_accuracy: 0.7078 - val_auc: 0.7656\n",
            "Epoch 100/100\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.5621 - accuracy: 0.7429 - auc: 0.7948\n",
            "Epoch 100: saving model to /content/drive/My Drive/models/ckpt_epoch_100.h5\n",
            "625/625 [==============================] - 3288s 5s/step - loss: 0.5621 - accuracy: 0.7429 - auc: 0.7948 - val_loss: 0.5821 - val_accuracy: 0.6812 - val_auc: 0.7445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = discriminator.evaluate(test_dataset) #evaluate model performance for unseen data\n",
        "logging.info(f\"test accuracy: {test_accuracy*100:.2f}%\")\n",
        "print(f\"test accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "p-D51KsPfBFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator.save('/content/drive/My Drive/final_discriminator_model.h5')"
      ],
      "metadata": {
        "id": "fo3DdywlfBuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YU2FXT3xpDyY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "#visualize AUC\n",
        "plt.plot(history.history['auc'], label='Training AUC')\n",
        "plt.plot(history.history['val_auc'], label='Validation AUC')\n",
        "plt.title('Training and Validation AUC')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize training process\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6ievPAaDk2Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate model performance\n",
        "train_loss, train_accuracy = discriminator.evaluate(train_dataset)\n",
        "val_loss, val_accuracy = discriminator.evaluate(valid_dataset)\n",
        "print(f\"Training Accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Training Accuracy: {val_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "EdbRVvXhk47a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions_and_labels(model, dataset):\n",
        "  predictions = []\n",
        "  true_labels = []\n",
        "\n",
        "  for images, labels in dataset:\n",
        "    preds = model.predict(images)\n",
        "    preds = (preds > 0.5).astype(np.int) #convert probabilities to binary labels\n",
        "    predictions.extend(preds.flatten())\n",
        "    true_labels.extend(labels.numpy().flatten())\n",
        "\n",
        "  return np.array(predictions), np.array(true_labels)"
      ],
      "metadata": {
        "id": "YgCya5gHk8i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, true_labels = get_predictions_and_labels(discriminator, valid_dataset)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
        "print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "GeFwGVuFk-d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(true_labels, predictions)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Sh-XCshTlA5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_deepfake(image_path, model):\n",
        "  img = image.load_img(image_path, target_size=(256, 256))\n",
        "  img_array = image.img_to_array(img)\n",
        "  img_array = np.expand_dims(img_array, axis=0) #expand dimensions\n",
        "  img_array = img_array / 255.0 #normalize from 0 to 1\n",
        "  logits = model.predict(img_array)\n",
        "  probabilities = tf.sigmoid(logits) #convert logits to probabilities\n",
        "\n",
        "  return \"fake\" if probabilities.numpy()[0] < 0.5 else \"real\""
      ],
      "metadata": {
        "id": "0LdQTu3NktN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMQxGyETuUQR"
      },
      "outputs": [],
      "source": [
        "saved_discriminator = load_model('final_discriminator_model.h5')\n",
        "\n",
        "# example usage\n",
        "# result = predict_deepfake('image_path.jpg', saved_discriminator)\n",
        "# print(\"The image is: \", result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}